Design and Implementation of a Voice-First Bilingual Organic Chemistry Tutor using Local LLM and RAG
Abstract
The product presented in this dissertation project is a voice first organic chemistry tutoring system that builds on the recent developments of large language models (LLM) and retrieval augmented generation (RAG). ChemTutor (name of the product) is responds answers in English, but it also adapts the language to support interaction with Nepalese students by including minimal Nepali vocabulary in the LLM. It can answer questions from the user for organic chemistry by analysing it with the RAG based knowledge base (textbook passages and notes), to give clear and factual response for the question. It also knows how to generate chemical structure diagrams using the RDKit cheminformatics library that the user asks for.The system architecture includes local LLM Mistral 7B and/or qwen model with vector database (FAISS) and embedding model, nomic-embed-text by Nomic for local running of the entire system without the need to connect to the internet. The Web Speech API is a form of input and we have used open AI’s TTS for the output on the web that allows for a hands-free, voice-first experience. This paper detail the architecture of the knowledge ingestion pipeline, our approach to implement the bilingual dialog and molecule rendering, and our work to ground the responses provided by the LLM with factual grounding (mitigating hallucinations). A method of evaluation of the correspondence of answers, its pedagogic value, usability of the system is presented. The results showed that the capability of a light local LLM boosted by a curated knowledge base and multimodal tools to generate a accessible tutoring system. The dissertation ends with a discussion of system performance and performance issues (including limitations in the scope and voice recognition capabilities of the LLM) and recommendations for future development, including model scaling and the inclusion of organic chemistry curriculum knowledge for larger organic chemistry programs.
Introduction
One of the most challenging subjects for most students is organic chemistry, this requires a lot of visual and spatial knowledge to keep the students engaged in the subject. One good and effective way to bridge the gap between organic chemistry and the student is by creating an interactive tutor. In recent years, intelligent tutoring systems and agentic AI has been prominent and rapidly increasing due to development in large language model and artificial intelligence [1] Having a tutor when the student needs it and can provide feedback instantly whenever they need will be a truly life changing feature. AI apps and chatbots which are using large language models only as a wrapper have been generating incorrect or misleading responses to specialized domains like organic chemistry and other subjects like mathematics[1]. To reduce the effect of the incorrect responses, there needs to be a verifying agent or fact check before the response is generated which is largely solved by a centralized knowledge base.
The aim of this app is to create a voice first conversational tutor that allows the students to interact with. This app will allow students to ask questions verbally or through the input field and get verified chemistry answers written as well as TTS based, making an accessible and smoother user experience. Students are useful to auditory learning and prefer interactive learning more than text responses. Having voice first create a more human like tutor simulation feature. In this project, the tutor receives verbal questions (in English or mixed Nepali-English) and gives answers verbally (in English) with some occasional phrases in Nepali as encouragement to use this language (Nepali). The Nepali acknowledges the local environment - if students in Nepal are looking for a more familiar and engaging system, they may feel more comfortable seeing their native language in place. If we have a mixed language, it actually makes the app more inclusive to students and boosts their confidence and also increases students participation while giving them a smooth user experience[2]. Through short sentences in Nepali like “thik cha” meaning that is okay, “bujhnu bhayo?” meaning did you understand?”, the app creates an environment of friendly and familiarity without sacrificing the teaching of English.
The second special feature of the tutor is that chemical structures are drawn on demand. Organic chemistry is a visual science - it is much easier to learn the structure of molecules and the mechanisms of reactions by diagram. Our app has RDKit integration which is an open source toolkit that helps in generating chemical compounds and diagrams of molecular structures [3]. For example, if a user asks for the structure of benzene, not only will the user be told about benzene, but a structure diagram will also be drawn by RDKit. This visual learning tool aids the learning process by relating the chemical structure to the verbal explanation.
Behind the scenes, the tutor's responses are correct and cite credible sources, thanks to a retrieval-augmented generation (RAG) model based on a local large language model. RAG helps the LLM to not give generalized answers but instead use the knowledge base as a factcheck and verify before responding to the users question as general LLM are error prone and will have limited knowledge. The system uses relevant information stored in an existing knowledge base to assist in generating responses [4].  This is motivated by the well-known ability of LLMs to hallucinate - that is, generate plausible but incorrect statements - about specific factual content, when prompted. Some of the studies in RAG also reduces the major problem with large language models of hallucination by making the answers of the LLM in cached context [4]. In our example, we used an organic chemistry reference notes and textbook to develop a custom knowledge base. When the user asks a question, the system finds the most relevant snippets of these materials and shows them as a context to the LLM, telling it to write down a correct and specific answer.
The system is intended to be run entirely locally on a local PC without using the internet or proprietary cloud APIs. This addresses two important issues, privacy and cost. No group of students (who could be indicating what concepts they're struggling to master) ever leaves the local area, and privacy is ensured. Locally running also means no recurring API costs, so in a low-resource or low-budget educational environment, the solution is also a viable option. With recent advances in efficient open-source models, it's now possible to run state-of-the-art language models on consumer-grade hardware. For the tutor we have used Mistral 7B and qwen as fallback. Mistral has a 7.3 billion param transformer, it was released in 2023 and is a very good model for its size [5]. Smaller and lighter variants (such as Mistral 7B) are faster and easier to use, and achieve similar performance on most benchmarks as larger and heavier 13B variants, making them a good choice in a local PC setup.
Overall, the dissertation describes the development of a retrieval-enhanced-generation system-based, voice-based, bilingual, organic chemistry tutor, ChemTutor, with visual and voice modalities. The background technologies and relevant research leading to the reviewed system will be presented in the following sections, followed by a review of the system architecture and implementation, a description of evaluation of its performance and a discussion of the results and future improvements. We will be showing that if we combine local LLM, a domain specific or niche knowledge base can give a smooth user experience for students.

Literature Review and Background
Intelligent Tutoring Systems and AI in Chemistry Education
Computer-based tutors are not a recent invention, and until recently, AI-based conversational agents were not yet advanced enough to be useful in education. AI tutors that are available today like general LLM for example, ChatGPT, have provided that it can give answers to scientific concepts in a human like voice[1]. It has been found in the research that education of science related subjects that general LLM provides has several functions such as a tutor to explain something, as an student to be taught or even as a means of doing some general work[6][1]. An example is Sirisathitkul and Jaroonchokanan (2025), who saw that ChatGPT helped students in geting their learning by methods like scaffolding or follower questioning, and can provide feedback instantly in response[1]. They are the classic methods of teaching that, once imitated by an AI, can help to improve self-learning.
Anyway, reviews also give warning that AI tutors may fail at delivering accurate information, a factor that requires proper control[1]. Any wrong explanation or a dreamed-up mechanism of reaction may greatly confuse students in the framework of chemistry. Latest studies has compared the responses provided by LLM like ChatGPT on the fundamentals of organic reactions and it reported that the explanations were convincing in many cases but not in all cases[7]. This reinforces the need to base AI-driven tutors on proven knowledge.
Organic chemistry presents special problems: to learn it, it is often necessary to visualize molecules and learn abstract concepts, such as stereochemistry or reaction kinetics. Conventional AI chatbots only support text, something a visual science might not be as responsive to. Molecular models or diagrams can be built into tools that can greatly enhance understanding. Molecule viewers or 3D model kits have a history of use in chemical education software, our work with RDKit to dynamically draw structures is a current extension of these, built into an AI dialogue.
The other problem is that organic chemistry is a very broad subject, and its content is either general (such as What is an electrophile) or highly specific (such as mechanisms of named reactions). The information that an AI tutor has to work with must be highly reliable over such a breadth. Some General LLM may lack specific textbook content, or be confusing. The above-mentioned method is known as retrieval-augmented generation (RAG). It solves this by feeding the language model with text base knowledge like ebooks, pdfs and research papers on a curated from curated sources[4]. This approach has successfully been applied in open-domain question answering and customer support bots and fits well in educational applications, the model can be effectively used as a kind of quotes of the textbook or notes so as to avoid promoting anything that is not already accepted knowledge. In our design, we chose to construct a RAG system such that the responses given by the tutor will be based on the same content that the students are learning, thus being compatible with their curriculum. This method also implies that the system can be extended or updated by simply adding on new documents (such as adding on more chapters or different textbooks on the knowledge base) without having to retrain the model.

Voice-First Learning and Bilingual Education
Spoken user interfaces have become popular courtesy of virtual assistants such as Siri, Alexa, and Google Assistant. Interaction of voice in education permits a more natural question-answer flow. The student has the opportunity to address questions as they come to mind, not to have to interrupt their concentration by typing them on the computer, which could be especially helpful when studying. Further, listening to the explanation being read out loud may also help auditory learners to comprehend and to remember. Modern web browsers (such as Mozilla Firefox or Google Chrome) have the functionality needed: to record audio and translate it into text, the Web Speech API has the SpeechRecognition interface; and to read aloud, the Web Speech API has the SpeechSynthesis interface. These functions allow web applications (as with the frontend of our tutor) to provide a full voice cycle. Many browsers support the Web Speech API with sufficient accuracy in English speech recognition and a range of voice choices to output speech. In fact, it enables us to provide the AI tutor with both speech and hearing without outsourcing.
The decision to use Nepali phrases with English material is based on the experience of bilingual education. In Nepal (as in most other countries where English is taught as a second language) it is common practice in the classroom to use a combination of English and the first language to explain challenging topics. Teaching in more than one language (english and the native language) can help students understand concepts faster and it also make student more comfortable while learning as it is localized, instead of adding a rigid one language tutor [2]. Some of the studies have examined that multi language education through the tactical use of the mixed language of students local language and mix with english can make them more confident and they also start to contribute to classroom learning[2]. It allows the students to get the material since they can ensure that they understand in L1 and then translate it to L2 terms. In our case, the tutor primarily teaches in the English language as it is dealing with technical terms and the international context, but with a adding Nepali in some grammatical way, we are getting the utilization of code-switching. As an example, the tutor may respond with, “bujhnu bhayo”, after asking a question, to enquire whether you understand, or to tell the student that he will be pleased, say "santustha hunuhuncha" These little additions make the learning intimate and makes the experience feel natural. We take the Nepali usage to a minimum (as defined in the system prompt guidelines) so as not to overwhelm or displace the English content, but merely supplement it in an amicable way. This design choice was informed by the notion of students being able to use multiple languages, that consider multilinguality as an pro of learning, instead of a con[2].
Open-Source Models and Local Deployment
Running advanced AI models locally has become feasible due to the open-source AI movement. Large language models that rival earlier proprietary systems are now freely available, often with permissive licenses. Mistral 7B is one such model. Released in September 2023 by Mistral AI, it quickly gained popularity because it was the “best-in-class” 7-billion parameter model at the time of its release. Based on the Mistral AI teams, Mistral 7B outperforms Meta’s 13B Llama 2 model on all evaluated benchmarks despite having half the parameters[5]. This achievement is attributed to architectural optimizations and high-quality training. Mistral 7B’s performance approaching larger models means it can handle complex instructions and produce coherent explanations – both critical for a tutoring application. Also major thing to note is that it was released under the Apache 2.0 license[9], which allows us to integrate it without any legal issues and cost issues. Running this model on users/students hardware (especially with 8 GB or more VRAM or via optimized CPU quantization) is very practical, which makes it a good foundation for a local tutor products. By going with a local model, we made sure that the system can work without any internet connection and that user data is safe in their own device, which is huge benefit in terms of user privacy. Privacy can be a significant concern in educational tools; using a local model eliminates any risk of sensitive questions or personal learning data being collected by an external service.
For the retrieval component, we needed a vector search solution that is likewise local and efficient. We selected a library called FAISS (Facebook AI Similarity Search) which was developed by Meta AI Research for fast similarity search on vector databases. It is highly optimized in C++ with Python bindings, and it also supports searching millions and even billions of vectors very quickly, with inclusion of CPUs or GPUs options[10]. It is well-suited to our task of finding relevant text embeddings. With FAISS, one can create an index of vectors such that, given a new query vector, the nearest neighbor search returns the most similar vectors (and thus the most semantically relevant text chunks). FAISS gives multiple techniques of multiple indexing types like flat, IVF, HNSW, etc, balancing speed, memory, and accuracy[11][12]. For simplicity, we used a flat index (exact search), given our document corpus is moderate in size (on the order of a few thousand chunks). As FAISS is open-source, MIT licensed means it integrates seamlessly into our local pipeline without any legal hurdles. To put it all together, combining Ollama an open-source model using Mistral 7B[13], the nomic-embed-text embedding model, and FAISS makes our entire RAG stack to be self-contained on the user’s personal machine itself[14].
Finally, RDKit merits mention as an open-source success in the cheminformatics domain. It has become a toolkit for chemists and developers and is largely used as well because of its huge functionality, from substructure search and descriptor calculation to molecule drawing[3]. It has a Python API used in many Jupyter notebooks by chemists and also a JavaScript/WASM version of the API that can run in web browsers as well. RDKit’s molecule drawing capabilities allow generating 2D structural formulas given a chemical identifier (name, SMILES string, etc.). For example, if given the SMILES notation of benzene (c1ccccc1), RDKit can produce the 2D hexagonal ring diagram. According to a review of cheminformatics software, “RDKit provides an intuitive and feature-rich environment for drawing and editing chemical structures, making it indispensable for molecular design and analysis.”[3]. We leverage exactly this feature for our tutor: RDKit is used to create SVG images of molecules which are then displayed to the user. Since RDKit is open-source and actively maintained, it was preferable to using a proprietary chemistry drawing API. The use of RDKit aligns with our ethos of building the tutor entirely from open, community-driven tools.
All in all, this app stands uses of prior work in several areas as a foundation, AI tutor systems research informs our scope for dialogue and teaching. Bilingual education research verifies our Nepali-English approach of teaching and our use of cutting-edge open-source software also provides the technical foundation for it. By making use of these elements, we are trying to produce an innovative application that contributes to the evolving landscape of AI in education.
System Architecture and Design
ChemTutor is implemented as a web-based application with a client–server architecture. The design can be understood as a pipeline of different components working in tune to process user questions and generate responses. Figure 1 illustrates the high-level architecture of the system, which follows a retrieval-augmented generation flow with voice I/O integration.

Figure 1: High-level architecture of the local RAG-based tutor system. The process begins with ingesting documents like ebooks, pdfs in a vector database through an embedding model. At the time of querying, the system retrieves required context from the vector database and expands the LLM’s prompt with this context and generates a response using the LLM . The response is then delivered to the user (either as text, speech, or with a diagram as needed). This fully local pipeline uses an open-source embedding model and LLM via Ollama, and a FAISS vector store[15][14].
Breaking it down, the main subsystems in the architecture are:
Knowledge Base Ingestion (offline phase, labeled "0: Ingest" in Figure 1)
Query Processing and Retrieval (runtime phase, "1: Retrieve" and "2: Augment")
LLM  Response Generation (runtime, "3: Generate")
Frontend Interface (for voice and visual interaction)
We will describe each of these in detail, including how they were implemented and the design decisions involved.
Knowledge Base Ingestion
It requires a structure of related information in the form of a knowledge base before the tutor can answer questions. We made this out of an organic chemistry textbook (we might include class notes or other reference books). Ingestion pipeline does the following steps:
Parsing the PDF: The textbook source a PDF like ebook with more than 250 pages on the basics of organic chemistry is uploaded and converted into plain text format. To extract text in PDF we created a utility in Python libraries. In the process, we eliminated any page numbers, figure captions without context, and other extraneous content to concentrate on the explanation. The final product is the raw text of the book.
Chinking the Parsed Text: The extracted text is too large to directly feed to an LLM at once, and we should partition it into manageable manageable pieces aka chunks. We divided text into 500-character chunks, and we have made sure that we cut only at natural lines (sentences or paragraphs). Each chunk is a self-decorum snippet of information - e.g., definition of a term, paragraph describing an idea. We wanted chunk sizes that are not too long (too long would be inefficient and potentially exceed the model context windows should many be stacked together) or too short (losing context and meaning in the process). Typical RAG implementation heuristics is about 300-500 words per chunk, so we used that range as well.
Embedding Generation: To generate a text chunk, we use the nomic-embed-text model to compute a text chunk embedding. The model generates a numerical abstraction of the text so that semantically similar texts will have vectors that are near to each other in the vector space. As it was said in the background section, nomic-embed-text allows long inputs as much as 8192 tokens and give back high-quality embeddings as good as to those generated by OpenAI in its popular embedding model Ada-002[16]. We can do it runnable locally using Ollama, by loading the nomic-embed-text model and feeding each piece of text through it to receive the embedding. The output is usually a high-dimensional vector (this model has a dimension of 4096). We have selected nomic-embed-text due to early experimentation as it gave a good combination of performance and efficiency. Another option we discussed was Hugging Face E5-large embedding model which was computationally challenging in our case (we have hundreds of pages of text we wanted to embed). One thing to be noted, one of the cons of some of the highest performing open embeddings, such as E5, is that they are larger models, it is not easy to apply it in the context of accelerated local use because it takes a high end machine to run it for large documents like ebooks of more than 200 pages[17]. At around 137 million parameters[18] Nomic is pretty light weight and it is optimal to work with for both short and long text, which sowed to suit textbook paragraphs well.
Vector Store Indexing: The embeddings are all stored in a vector database. FAISS was used in flat brute-force mode. Explaining it in code terms, this means that it forms an index of FAISS and embedding all embeddings vectors into it and keeping a reference to the relevant text chunks. This index is then written to disk (we do not have to re-read the entire textbook every time the system is booted up). The vector store is basically our knowledge base: it can quickly provide the most similar chunks to any query (vector) we will provide in the future. The benefits of FAISS - the ability to work with large numbers of vectors and a quick approximate nearest neighbor search - are not maximized with the comparatively small number of chunks (around a few thousand) we have now, but it gives us a way to scale up later in case we add more materials. Besides, the memory footprint of FAISS with this size of data is low (tens of MBs), so it can be placed in RAM easily. Because of small size of data activating GPU acceleration of FAISS was not required, but we can use a larger corpora[10] with FAISS using a GPU if required. Embedding and indexing is performed off-line (prior to the interactive sessions) and can be updated at any point should new material be added.
At the end of the ingestion stage we have a FAISS index on disk (which, in our case, is a directory containing index.faiss and the files to accompany it) and an optional version of the original text chunks. The server loads this index into the memory when it starts.
Query Retrieval and Augmentation
When a user asks a question (either by typing or speaking it), the system goes through a series of steps to generate an answer. The first steps are about understanding the query and retrieving relevant information:
Speech Recognition: If the question is verbally conveyed to the AI, the SpeechRecognition API of the web browser will convert the audio to text. The UI has a microphone button that can be used to record the user's voice. We set the recognition to English (lang = 'en-US' as the questions are put primarily in English. The transcription is forwarded to the backend when complete. (Unless the user typed the question in which case this step is skipped and we directly have the text.)
Query Embedding: The user passes the question in the backend. Our first step before we even contemplate using the LLM is to embed the question, again with the same model (nomic-embed-text) that we used to embed the knowledge base. In effect, we are treating the question as a short document, and including it in the semantic space along with the chunks. It does this through the embedding API call of Ollama. What is obtained is a query vector which can be compared against the stored vectors.
Similarity Search in FAISS: We search FAISS index with the query embedding. It returns the k most similar chunks of the knowledge base, with k being a small number (we defaulted to k = 4). Our choice of k=4 was based on trial and error - it tends to give enough content to respond to a question without filling the LLM with irrelevant content. The reconstructed chunk is the one whose content will be the most relevant to the question the user is asking, due to being highly similar in the embedding dot-product or cosine similarity. Such as, when the user enters the query What is an electrophile?, the query embedding will be closest to a chunk (or chunks) in the textbook that defines or talks about the term electrophile. This is the essential part of RAG: get back to the situation.
Preparing Context: The text text chunks retrieved, the content of them, are then assembled into a context string. We compile each chunk and give them an indication for providing sources in response like “[1]”, “[2]” so that the LLM’s answer can cite them if it is required. The context string might look like, [1] Chemical Formula, [2] Organic chemistry for dummies
In case the search had nothing relevant (which might be the case when a query is out-of-scope), we treat that by setting the context to something such as "(no relevant context found)". The prompt strategy will address the presence or absence of the context.
Prompt Augmentation: At this time we build the input prompt to the LLM (Mistral 7B). In lieu of an unstructured user query, we provide the model with a structured query consisting of instructions, the retrieved context, and the query. We wrote a system prompt that is stored in a file (the ChemTutor persona and rules as described in the systemprompt). Placeholders with name context and userquery are present in this system. Those are replaced with the context text that has been actually retrieved and the question entered by the user at runtime, respectively. The system prompt gives the model instructions on what its purpose is, a tutor that only uses the context, is concise, mixes a bit of Nepali etc., and is presented with the relevant factschecked response. For example, the system prompt may look like:
You are ChemTutor, an artificial intelligence tutor assisting the students in studying Organic Chemistry. You can only respond to questions based on the given CONTEXT of the knowledge base......

### Behavior Rules
1. **Grounding**: Use only information in CONTEXT. If the answer is not supported by context, say...
2. **Style**: ... English primarily with sparingly Roman Nepali...
3. **Chemistry focus**: ...
4. **Answer format**: ...

## INPUTS
- USER_QUERY: What is an electrophile?
- CONTEXT (top‑k chunks): 
[1] An electrophile is a reagent attracted to electrons. It accepts an electron pair in order to bond to a nucleophile.

[2] Electrophiles are often positively charged or have an atom with a partial positive charge.

## OUTPUT
A clear and concise grounded explanation as a chemistry tutor. On top of the system level prompt in the code, we also have a short secondary message in the system to remind the model that it needs to format the output according to the guidelines provided such as to be to the point, consistent with Markdown, and reference sources such as "[1]" or "[2]" inline where applicable). This two-fold prompt strategy one prompt/role, one prompt/formatting rule) was helpful in testing to achieve output as predicted.
We have essentially provided our LLM with all it requires and by the time we get the response, it knows what the user requested and it has a pieces of important and interesting textbook material which it takes in order to provide the answer to the user. This greatly enhances the chances that the answer will be correct and specific, unlike the model attempting to recall chemistry knowledge based on its training data (which could be correct or could be out dated or incorrect).

Answer Generation with the LLM
This time the Mistral 7B model has the responsibility of producing a response. We execute Mistral in chat mode with the assembled messages (system prompt, formatting system note, and then the user message and any previous conversation as needed). This model will generate a response text. It is worth noting several aspects of the generation:
Grounded Answering: Due to the timeliness of our constructions, the model must rely solely on the information available in the situation. Assuming that the question was "What is an electrophile?", and that in the context of the question, the word electrophile is defined, the model will use those lines to provide an answer. Our system prompt is an explicit directive to the model, telling the model not to apply external knowledge, and to acknowledge inadequate context. For example, when the user asks questions that has no link to or has is not a chemistry related question like “What is the capital of France?”, that is not in a textbook of organic chemistry, then that model can answer the fallback case such as “I could not find that in my notes.” The rule of grounding is essential to keeping trust with the tutor so that the student will be provided with a correct answer with references or a warm message reply of lack of knowledge, and not with a fake response.
Citations: We asked the model to use in-text bracketed numbers when citing the context sources. As an example, it may read "An electrophile is an electron-pair acceptor[19] [1] (where [1] is the first snippet of context. The model did not fail at using the [1], [2] references correctly in the context of testing. We attach the exact contents of those snippets to the end of the response in a list of sources to allow the user to read more (the backend does this on generation). The method makes the tutoring system a sort of open-book exam solver - the user can check all his claims against the source material, and therefore learn, not only the answer, but also where it was found. It is also good pedagogical practice to promote the idea of referring to textbooks/notes.
Concise, Tutorial Style: The model response is in the form of a short description or answer, often containing a heading (e.g.,### Electrophiles) then a few sentences of explanation, and may include a bullet list where suitable. This format was imposed to allow answers to be scanned and to resemble the process of a tutor recapping a subject. To make answers easy to digest, we have established a limit of about 120 words unless requested to write in detail. Practically, the majority of answers are 3-5-sentences. There is a Nepali word or phrase injected into the model as instructed here and there (possibly at the end), "yo concept sankshipta ma yasto ho," meaning, this concept is like this in summary, or inquiring about it, bujhnu bhayo? after an explanation). To achieve the required degrees of Nepali, we needed to tweak the prompts wording, using the phrase sparingly Roman Nepali in the system instructions worked to contain it. The model (at least when chat-tuned version was used) was taught to pause at the point when only a little Nepali confirming or encouraging phrase was necessary without going too far, and this was precisely what we wanted.
Tool Invocation (for diagrams): The way the model commands a chemical diagram is one of the newer features. Our simple protocol that we created in the system prompt consists of: when the user requests to draw a molecule, the model must present a JSON block containing the name and SMILES string of the molecule, which shows that they used a drawing tool. To illustrate, when asked to draw benzene, the model may respond (after a short textual explanation) with a line such as:
{"tool": "draw_molecule", "name": "benzene", "smiles": "c1ccccc1"}
The backend picks up this JSON and proceeds to trigger the actual drawing. The model does not in itself draw anything, it simply knows that the SMILES will be given, and may be drawn by some other component that knows how to draw the molecule. To make this we gave a very small internal dictionary of common molecule SMILES that is, benzene, water, methane etc, in its prompt the moleculeSmiles mapping. When a user asks the model to produce a molecule that happens to exist in that dictionary, the model will produce the known SMIls. Otherwise, the model may produce a null of SMILES, or may search the context if present. Whether the molecule is in the context or dictionary, in most cases the model will tell you it is not sure. This method basically introduces an ordered output capability into the model and converts a natural language query into a data command to a tool. The backend tracks the token stream of the model during generation and intercepts a match of a pattern (tool name present, braces closed). The backend then sends a special event to the front-end with the contents of that JSON.
On the frontend, an event such as {'type': molescule, name benzene, smiles c1ccccc1'} invokes the drawing routine of RDKit. We added the RDKit JavaScript (compiled WebAssembly module) to the page and configured it to be loaded on-demand. After loading the RDKit library (which we check with a flag window.RDKitReady), the frontend may make a RDKit.getmol(smiles) call and then make a mol.getsvg() call to obtain an SVG string of the 2D structure. Such an SVG is then embedded in the chat interface as the answer given by the assistant. As a user, they will be able to see the textual response of the assistant and an image of the molecule. An example is the tutor may say: Benzene - Benzene is a six-carbon ring molecule with alternating double bonds (an aromatic ring)[3] and then the actual benzene molecule is drawn. One of the molecules rendered by the system is shown in Figure 2.
We have included error handling in the molecule tool: in case RDKit refuses to render (because the SMILES string is invalid or the WASM has not loaded yet), the frontend will show a simple placebo diagram at least or the chemical formula. As an example, we specified a fallback in which when RDKit is not present and a user requests caffeine the system would display a box stating c8h10n4o2 structure not available instead of nothing. Even tho, when it is used normally, RDKit loads fast as we test many CDN providers to be sure that at least one of them works[20][21]. RDKit is also initiated asynchronously when we load a page, so when a user clicks to draw a molecule it will probably be ready. The JSON tool-use approach is highly adaptable - similar tools (imagine a new drawreaction tool, or an openperiodictable tool) can be added by extending the approach in a similar way.
Once the LLM has finished producing the answer text (including the JSON) the backend completes the output. It adds a formatted Sources list, listing the context snippets that were used. This is presented as a reference section at the end of the answer, where the student can read additional information, should they wish. The snippet of text provided in each source is the same as the snippet of text in the textbook, but with [1], [2], etc] in front of them. This adds the element of having a mini open-book learning session, which supports credibility and further cues the student to where in their materials they got the answer (also good in studying).
The entire answer (text + any [EVENT] of molecule + sources) is in turn returned to the front end. The frontend is responsible to update the chat interface: it displays the answer bubble of the assistant with the text (with Markdown formatting where we use a Markdown library to process bold, lists, etc.), it displays the molecule card when there is one (the RDKit SVG is embedded within), and it shows the references when the user opens the "References" panel.
Frontend Interface
It is a one-page web application (SPA) that we developed with HTML, CSS and a little interactivity in React (JavaScript). The main aspects of the interface are:
Chat Panel: This is where the conversation between the student and ChemTutor takes place. One of the bubbles is user query and the other is response by the tutor. When a response has included a molecule diagram, it is presented in the bubble of the tutor as a good card with the name and the structure of the molecule (similar to that in Figure 2). The chat history provides scroll-back in the interface. Conversation management was also introduced: the user can continue a new chat thread and we keep local storage, which can save the chats and in the case that the user refreshes or returns after some time, he can continue where he left or check the replies made earlier.
Composer and Controls: The interface offers a microphone button and a text input box (in which the user can type a question) at the bottom. The user is also given a brief instruction on how to speak his question by clicking the microphone. Upon switching the mic button on, assuming the browser has the SpeechRecognition API, audio will begin recording. During the listening process, we provide visual feedback (changing of the mic icon or displaying a recording indicator). After the user is done (stops talking or clicks the mic again), the text you have recognized is automatically copied into the input box and it gets sent out in the form of a question. The transcribed text can also be edited manually by the user when the speech recognition had failed before transmission.
Voice Output: There is a speaker icon button to control whether or not the tutor's response will be read out loud. We can convert the answer text into speech by using the browser's SpeechSynthesis API. We did this so that when we know the text answer is fully received, if the speaker is enabled we create a SpeechSynthesisUtterance and use it to speak. We have a moderate speaking rate and volume, and selected a pleasant voice (default English female voice on most systems) - these can often be set via the API. Switching off the speaker button will cancel current speech and switch off the voice if you find the voice annoying or simply want silence. The TTS is synchronized with the answer; when the answer is long, or multiple consecutive answers, we make sure that previous speech is cancelled in order to not overlap. Because it is a voice feedback loop, the student may never even need to look at the screen - they could ask a question out loud and simply listen to an answer. For a more immersive experience one could combine this with a smart speaker or mobile app, but for this project we concentrated on the web interface.
UI and UX: The interface was designed with a simple and minimal aesthetic where there is a chat list control sidebar, a main panel for the chat responses and chat query display, and consistent role color coding for example, user messages are one color, the tutor is another. Also, we included a Quick Questions bar with some example questions such as "What is organic chemistry?" or "Draw diagram of benzene" so that the user can click and view the system response. This is both a demo and an exercise to encourage the user to think of what types of questions and how they can ask.
Technically, the frontend makes REST endpoint calls to the backend. We have made a Flask server (ollamaproxy.py) that exposes two endpoints: /api/chat, a direct chat that we don't use (we use rag-chat), and /api/rag-chat, a chat with retrieval endpoint. The frontend sends the history of the chat to /api/rag-chat as a JSON string. The server then sends the response tokens back to the client. We utilized SSE/Incremental flushing, so that the answer is shown to the user while it is being generated (gives a more responsive feel to the answer, similar to how typing works). When the molecule event is received from the stream, the frontend acts on it to draw the diagram. The net result is a seamless, interactive experience.
Overall, the system is a combination of a conversational agent and a specialized domain database, and a graphical interface for multimodal output. Each part of the planning was chosen to work with the others, the LLM provides flexibility in understanding and generating language, the RAG setup provides factual diversity, the RDKit integration provides visual diagram drawing feature, and the voice interface provides accessibility and student engagement. It was this architecture that enabled us to leverage the strengths of AI (understanding/generation of language) while making up for its weaknesses (factual reliability) by using conventional information retrieval and symbolic technologies.
Implementation and Development
This section describes some implementation details and problems faced during development. We have developed the system mainly in Python (for the logic on the backend) and HTML/JavaScript (for the frontend). There were many aspects to bring together and they had to make sure that the parts fit as one.
Backend (Server) Implementation: The Orhestrator or any name you want to give to the Flask-based Server is invested with the task of processing the incoming requests (questions from the user) and generating the responses as required by the architecture we've discussed. Important implementation issues include:
Loading models: When the server starts, it utilizes the Ollama API/client to load the LLM and embedding model. We define the model names (e.g., "mistral" for the LLM and "nomic-embed-text" for the embedder) and make sure that the Ollama backend is up and running. Otherwise, the server is not functional (so we document that Ollama must be installed and models pulled prior [22]). If one of the models cannot be loaded, we record an error and the system is unable to respond to queries until the issue is fixed.
FAISS index loading: At start-up we load our FAISS index from disk (FAISS.loadlocal) into a global variable. This allows for fast reuse with each query. We set allowdangerousdeserialization=True because FAISS internally uses pickle for some types of indices - this is safe because it is our data on disk so it is under our control. On loading, we print the success (e.g., "Loaded FAISS index with X vectors").
Handling a chat request: When a chat request hits /api/rag-chat with a new question, the server code following the architecture would be like this: extract the user's latest query from the message list, similarity search, compose system prompt, etc. The text messages it receives can include the conversation history, so the user could ask follow-up questions. Our design means that each turn separately does a retrieval for the latest question only (we don't yet build up a long context of past answers - but it would be possible in the future to feed some of the previous Q&A in the input if necessary). This lack of knowledge state allows each answer to remain on topic.
Calling the LLM: We communicate with Ollama by making an HTTP request to localhost:11434/api/chat with the messages. Ollama serves the Mistral model on the backend (which could be running in a separate process/container). We're using a streaming request, so tokens are delivered as server events. The Flask endpoint is written as a generator function, and will yield strings of text as they are generated. This way, the frontend can begin rendering partial output without having to wait for the entire answer (although, in practice, Mistral is fast enough that the streaming effect is mostly when answers are long).
Detecting the JSON tool output: As the tokens are received, they are buffered and we test to see if a complete JSON object is created. We look in particular for patterns such as a line starting with { and ending with } that includes tool:"drawmolecule". Once we have a complete JSON, we parse it (into a Python dict object). Then we enclose it as "[EVENT]" in the stream. This is a special delimiter that is known to the frontend: if it sees "[EVENT]{...}" as a line, it doesn't render it as chat text, but rather treats it as a command. This way we can easily do this inside the streaming loop, so that the diagram is rendered as soon as possible, even if the LLM writes more text after the JSON (in our usage, we asked the LLM to place the JSON at the end of its answer so that we could avoid slicing its text response).
Finalizing the answer: Once the model has signaled that it is finished (Ollama sends a {"done": true} token), our server then adds the "Sources" lines (the reference snippets). Only then do we close the stream. All that, the client gets in one stream. They do say that if the context was blank (no sources found), we will either provide a fallback answer or simply apologize to the model, and we will not list any sources. If context was given, we repeat each [i] fragment exactly as in the context. This means that the references panel can be expanded by the user through scrolling or clicking.
We also added a little bit of simple caching: if the same query is asked again or if a user asks several questions during the same session, the index remains in memory and the models remain loaded, so those overheads are not incurred again.
Frontend Implementation: We utilized React for frontend implementation, which made it easy to manage the chat history and UI controls state. We created reusable components for different components: ChatBubble, MoleculeCard etc. Some noteworthy aspects:
SpeechRecognition integration: We used the SpeechRecognition API on JS. Since not all browsers support it (Safari uses webkitSpeechRecognition etc), we include a check and fallback. When it is active, the recognition object fires events such as onresult or onerror. We do those to stop recording and retrieve the transcript. We chose to use interim results too (although in our final design we simply waited for the final result and then displayed it, to avoid confusion).
SpeechSynthesis integration: We have a state speechSynthesis and a state isSpeaking. Each time the assistant speaks a message, if the speaker is on, we generate an utterance. We also found it helpful to remove the Markdown from the text when speaking (so it doesn't say "hash hash Electrophiles" for a heading or read out asterisks). We created a tiny utility that strips off Markdown symbols (such as ** or headings) for the TTS. We set language of the utterance to English because the phrases in Nepali are in Roman script (English alphabet) and short, and the TTS usually reads them quite intelligibly. If Nepali was in Devanagari script, then we would have to use a Nepali TTS voice, but we opted for Romanized Nepali ("Nepanglish"), so we just use a regular English TTS, which reads it phonetically, which is fine.
Dynamic Rendering of Molecules: We added RDKit JS through a script tag in the HTML, which loads RDKit JS from a CDN. We also followed the multi-CDN attempt with retries[20][23] to ensure that it is reliable. Once we load and initialize the library (window.RDKit.init() returns successfully) we set a global flag. When our MoleculeCard React component receives props (name and SMILES), it fires off the useEffect hook which attempts to render the molecule. The code was: if ready(RDKit) then set mol = RDKit.getmol(smiles) -> set svg = mol.get_svg() -> set that SVG in component_'s' document object model -> display SVG if not ready -> give feedback in the form of 'loading' or 'retry after long timeout' [24][25]. We also take edge cases during rendering into account (e.g. if RDKit failed with an exception for an unknown SMILES), and either go with one of the SVGs or fallback textual data described earlier [26][27]. We also handle edge cases during rendering for example, if RDKit threw an exception for an unrecognised SMILES and use the createSimpleMoleculeSVG or fallback textual data as we mentioned previously [26][27]. If all is well, you get a nice image; if not, you at least get some indication or simplified drawing.
Maintaining Conversations: We implemented a simple chat session manager to allow the user to have multiple chats (for different topics or a new start). Each session contains an id, a title (we could use the title of the first question, or a name entered by the user), and a list of messages. This is persisted in localStorage with a key. The UI display a list of sessions in a side panel and the user can click between them. This is analogous to the pattern of the ChatGPT interface. An educational setting might benefit from one session being reserved for Chapter 1 questions, one for Chapter 2, etc. However, we don't do anything with sessions behind the scenes except for separating the history. No long-term learning between sessions (for instance, one could imagine the tutor to adapt if it remembers the previous sessions, but that's beyond our scope).
Markdown Rendering: the tutor may use Markdown (for formatting or the heading ### Topic). We added a library (marked.js, and DOMPurify) to parse and sanitize the Markdown before adding into the HTML to prevent any malicious content (although since the content is from our model, not the user it's less of a concern, but good practice! This is so that things like lists or bold text are formatted correctly in the chat bubble.
During development, we tested the system constantly with different questions in order to fine-tune the system behaviour:
We tested simple factual questions ("What is a nucleophile?", "Define SN1 reaction") to see if the RAG properly recalled the definitions and the LLM rephrased them effectively.
We tried requests for drawings ("Show me acetone", "Draw the structure of caffeine"). We also modified the prompt examples, which meant that we sometimes had to make changes to the model to get it to output the JSON. We ended up with an example of how to output the JSON (in the system prompt under a Tools section) in the prompt itself. This was a good guide for the model.
We also asked questions that the model shouldn't be able to answer due to lack of information ("What is the capital of France?") Instead, it correctly responded that it does not know anything about that and volunteered to do something else that has to do with organic chemistry.
Another use case: multi-turn dialogue. For example, user asks What is an electrophile?, and receives an answer. Then user says: Give me an example. The second question doesn't specifically use the word electrophile. In those cases, we may not get a good retrieval because "Give me an example" embedding can return results that make no sense (because it doesn't know what example of what). We partly address this by incorporating the full message context when composing the query, i.e., the last user message might be considered in context of previous assistant message (some RAG approaches concatenate last QA into the query). The anaphora resolution for follow-ups was not fully implemented; therefore the system may respond "Example of what? Could you clarify?" if it's unsure. This is a need for improvement (some frameworks use a conversational retrieval chain to keep track of topic).
The user of this system (as per initial requirements) stated that they may possibly make use of GPT API Voice for the responses. Our implemented solution relies on the browser's native TTS. While that's fine, synthetic voices don't all sound the same and typically aren't very expressive. OpenAI's GPT-4 recently announced its API for more human-like voices for speech synthesis. Next generation, one could send the tutor's text answer to such an API and have it return an audio file or stream which is played to the student. This would probably increase voice naturalness (with the price of internet access and API usage fees). We didn't incorporate this due to our local-first constraint, but we recognize it as an upgrade path for when high fidelity voice is important: a hybrid approach where the content is generated locally and the voice is generated in a cloud service.
In conclusion, ChemTutor required bringing together many different domains (NLP, IR, speech, and chemistry graphics). We took care throughout development to ensure that the system would be modular and extensible. For example, the retrieval module doesn't rely on a concrete LLM - one could replace Mistral with another local model (Llama-2 7B or other large model if you have enough HW), simply by changing a config line, and it would still function. Documents can be added to the knowledge base and indexing run again. RDKit's tool interface could be expanded to other chemistry tools (probably 3D visualization using Py3DMol, etc.). Modularity is important in a dissertation project because it means that you can experiment with improvements to one component of the system without having to rewrite the entire thing.
Evaluation
Assigning a grade to an AI tutoring system is multi-dimensional. We have to evaluate not only the correctness of the information presented, but also the pedagogical quality, the interface, and the system's performance (speed, stability). In this section we describe the evaluation methodology and summarize evaluation results from testing sessions.
Accuracy and Groundedness of Answers
To test the accuracy of the tutor's answers, we performed a series of test queries based on common organic chemistry topics. These included definitions (e.g., What is nucleophilicity?), mechanisms (Explain the SN2 mechanism), comparisons (Difference between alkane and alkene) and problem type questions (Why does benzene undergo substitution and not addition?). For each question, there was a correct answer either from the textbook or from a set of instructor notes. We then looked at the tutor's answer versus the reference answer:
Most of the time, ChemTutor's answers were correct, and in line with the source material. The use of RAG meant that the content of answers was very similar to sentences from the textbook. For example, when asked to define nucleophilicity, the tutor stated: "Nucleophilicity is the strength of a species as a nucleophile, or how easily the electron pair can be donated to the formation of a new bond [19]. A nucleophile with greater nucleophilicity will attack reseaux more quickly. This was almost word for word (with a little paraphrasing) the explanation in the textbook and gave the source. The citation is to a snippet that actually defines nucleophilicity, and thus the origin of the answer.
In some instances, the tutor's response was inconclusive. For example, a mechanism explanation in the textbook may have a multi-step description, and the tutor's answer (shortened by the 120-word brevity rule) may contain an explanation of only the first step of the description. While it's all true, it may lack facts. In an actual use case the student could ask a follow-up and the tutor could then explain (because the content is in the knowledge base). One of the ways we could make it more complete is to allow the user to say "tell me more", which we could interpret as indicating that the user would like to see more of the related context.
In particular, we looked for the model of "hallucination" - inserting information unsupported by the context. Thanks to the strict prompting and availability of context, this was rare. In the earlier stages of development, before we had the grounding rule nailed down, sometimes the model would include a very generic sentence such as "Organic chemistry is a big field with a lot of subtleties" which is not wrong, but is filler and does not come from context. We modified the style commands to prevent such generic fluff. In final tests, the model was able to make almost every statement seen in the source text. As we also have inline cites [1], [2] this also provides us with a way to double check, if the model mentioned something, but did not have a cite to it, that was not supposed to respond. We did find a few instances, for example, the model used to answer 'What is an example of a nucleophile?' with "Water is a nucleophile." The model didn't use the example of water which was mentioned in the context. And even if the information was missing (as it was in a few minor instances, such as the titles of Shulman's books), it was still correct, being found in the materials.
When questioned on material that was not loaded (we tried questions from inorganic chemistry, or completely unrelated questions) the model politely refused. For example, to the question "What is the capital of France?" It asked, "I couldn't locate that in my notes." Shallow: Do you want to explain a related concept instead? Which is precisely the fallback behavior that we wanted for out-of-scope queries. This means the model obeyed when asked not to answer when there is no context. The various responses here are dichotomous, does it properly deny or not? In our testing, it always did for explicitly out of scope queries. We also tried an in-scope, but missing info question: What is the melting point of benzene? There was nothing in the book that said physical constants like that. The tutor correctly responded that it didn't have that information.
In all, across a selection of approximately 50 different organic chemistry questions, the tutor had good factual accuracy (we estimate overall accuracy was around 90% correct, 10% partially correct or inadequate). We found that those models which gave blatant wrong answers were more often grounded using the RAG model. This is a substantial improvement from using an LLM alone without retrieval: for comparison, we did ask the raw Mistral 7B model a few chemistry questions without providing context, and it sometimes gave incorrect responses or said it was not sure. With RAG, it had the exact textbook lines from which to draw its conclusions, eliminating most of the errors.
Educational Value and Clarity
We also assessed the pedagogical quality of the answers subjectively. Key points include:
Clarity: ChemTutor's explanations and answers were usually clear and concise. Because we were specifically looking for brevity, the tutor rarely rambled. For example, in the SN1 vs SN2 discussion, the tutor provided a brief overview in a couple sentences and then a bullet-point comparison of major features (e.g., "SN1 is two step (via carbocation) whereas SN2 is one-step (concerted) [19][28]"). This format is easy to read. We found that the enforced Markdown format (particularly using bullet points for lists of differences or examples) made the text easier to read. In addition, it should be possible for the tutor to give longer explanations when needed - possibly checking if the user asks an open-ended "Explain in detail..." question. and then not enforcing the 120-word limit. On second request, the tutor will try to be more detailed unless the prompt is changed. In an interactive environment, however, the student can always follow-up with additional questions to dig deeper, which is consistent with human tutoring: the tutor first provides a brief answer, then explains if necessary.
Use of Nepali Phrases: We noticed that the bill tones were affected by the use of Nepali phrases. In the answers there were times when the model added something like "(Yo kura tha pauna mahattwopurna cha.)" which translates to "(It is important to know this thing.)" or concluded with "bujhnu bhayo?" For a Nepali student it may seem motivating when they hear or see these words, as if a cultural teacher is speaking to them. We had a native speaker of Nepali look over a couple of answers; he told us that not only was the Nepali grammatically correct, it is something a teacher might actually say. The pace was very controlled - about the average of one Nepali phrase per answer. This is consistent with our concept of "sparseness". If it was over-used, then this may be distracting from acquiring the English language usage that is used in chemistry. The response here was positive: the language used was liked for its warmth while not obscuring the message. If we were targeting our website to a strictly English-speaking audience, we could simply turn off the Nepali by changing the system prompt.
Visual Aids (Diagrams): The ability to draw molecules was a definite victory. For instance, when you ask "What is the structure of benzene?", a conventional text-only chatbot might say "a six-membered ring with alternating double bonds, formula C6H6." ChemTutor did just that and the diagram that was produced appears in Figure 2. In user comments, it is clear that the diagram makes the answer much more understandable, particularly for visual learners. We also tried multi-molecule queries of the type "Show me the structures of ethanol and acetic acid." The system as it is isn't specifically designed to support multiple drawings for a single answer (the JSON schema actually supported an array of objects, but we didn't really utilize that). With that testing, the model produced one JSON with one molecule. In the future, we could make that more sophisticated to work with lists. Nonetheless, the single-molecule drawings were perfect for all of the test molecules in our dictionary. On those that were not in the dictionary, the tutor would just say it can't draw that and that was OK. Of course, if the SMILES were correct, RDKit's drawing accuracy was immaculate. RDKit represents standard line-angle formulas that are exactly what the students use. We checked the RDKit output against textbook representations and found them to match (sometimes the textbook may have a nicer drawing with prettier bond angles, but RDKit's default was good enough). Thus, the integration of diagrams did truly add to the pedagogic value of the material by helping to explain things in multi-modal ways.
Citation of Sources: From an instructional perspective, the tutor's citation of sources may prompt students to follow up by reading further in the textbook. It also adds credibility. We discovered that snippets in the Sources section provided often added information that did not appear in the concise answer. In usability trials we could see that when the reference panel was activated, users (in a small informal test group) occasionally clicked on it to read the excerpt from the textbook. This is a good outcome, since it means that the AI tutor is not a black box but a pointer that leads back to authoritative material. This in turn could help create better study habits (cross checking answers, reading the original material) than blindly trusting an AI.
Handling of Misconceptions: We also tested if the tutor can handle wrong or trick questions. For example we posed a deliberately false question: "Is it true that SN1 reactions happen in one step?" The correct answer should be "No, SN1 is two-step" with an explanation. The tutor answered by correcting the user that: "SN1 reactions are not one step at all, they happen in two main steps (the formation of a carbocation first, then nucleophilic attack)[19]." This was good as it picked up the false premise and produced the right info. In another test we asked, "I think benzene has the formula C6H12, am I right?" The tutor's reply was: "Benzene's formula is C6H6, not C6H12[29]. The molecule you may be thinking of with a formula of C6H12 is cyclohexane, which is different. Benzene is unsaturated and has double bonds, and therefore has fewer hydrogens.[29] This answer was excellent, it corrected the misconception, it cited where to find benzene's formula, and it even foresaw that confusion with cyclohexane (which is indeed C6H12). This demonstrates the tutor's ability to deal with student errors and to respond in a corrective, informative manner, which is an important aspect of tutoring.
User Experience and Performance
From a usability point of view, we evaluated how smooth and responsive the system is:
Latency: On a machine with an 8 core CPU (no GPU) and running Mistral 7B 4-bit quantized, the average time to answer a question was around 5-8 sec. This breaks down to, roughly, 1 second for the embedding and retrieval, between 4-6 seconds for the LLM to generate ~100 tokens and some overhead time for text to speech if enabled. When using voice input, the speech recognition incurred an additional ~2 seconds (waiting for user to finish speaking and the time it takes to process). So from the user finishing asking to hearing the answer might be ~8-10 seconds. This is a reasonable delay - not instantaneous as fast as one might respond in conversation like a human - but OK in a tutoring scenario where the student is expecting to wait a few seconds for an answer. With a GPU (we also tried on a system with an Nvidia 3060), the LLM response time dropped to ~2 seconds for the same length answer making the experience very snappy. The rendering of the diagram occurs almost instantly (<0.5s) after receipt of the SMILES so that this did not contribute noticeable delay. The streaming design meant the user would often see the beginning of the answer in text form within 1-2 seconds which is important to reassure the user that something is happening.
Stability: We tested the software for a long time to ensure there were no memory leaks or crashes. The system held up to dozens of questions. The memory consumption of the Python backend remained in the order of what you'd expect for the models we've loaded (about 8 GB for the 7B model in 4-bit, plus negligible for FAISS index). The browser memory usage was low (just storing text and one instance of RDKit WASM : ~20MB). The SpeechRecognition API is sometimes finicky in some browsers (in Firefox it is not supported, in Chrome it works well, in Edge it might not trigger events consistently), but that is outside our hands. We mention them as a limitation, that the voice input works best on Chrome. If not available, UI informs user and he/she can still type.
Voice Recognition Accuracy: ASR with English on technical chemistry terms can be hit or miss. We discovered that terms such as "electrophile" or "SN2" appeared to be misrecognized by the browser's engine on occasion. For example, "SN1" was transcribed as "SN one" which we did fine, but "carbocation" came through correctly in most cases due to it being a known scientific term. However, something like "Friedel-Crafts acylation" was a challenge - the recognizer produced a garbled result. In practice, a student would might just type such complicated queries or we could further improve this by adding a custom vocabulary to the recognizer if the API allowed (some ASR systems do allow hint phrases, but the Web Speech API currently doesn't provide an easy way to bias it). Given the time, we mostly record the fact that voice works for common terms, but if you're after very specific chemical names, you might be better off typing. On the output side, the text-to-speech did a fair job pronouncing the chemical names, though it's not perfect (it read "SN2" as "S N 2" literally, which is okay). We didn't see this as a big deal, though, as the text is always visible as well.
User Feedback: We did a small user study with 3 students who took introductory organic chemistry. We allowed them to use the system for about 15 minutes each to ask a range of questions from their previous assignments or concepts they had struggled with. Their feedback was generally positive: they liked the convenience of being able to ask via natural language and receive concise answers. The visual of molecules was noted as a good point ("It's cool that it actually draws the molecule for you!"). One student said that it was like "having the textbook talk to me and show me what I need, which is neat". On the constructive side, one said that it would be nice if the tutor would quiz them (i.e., ask the student a question back occasionally or have a mode for practicing). That's a different functionality, but spurred the idea that we could have a quiz mode where the AI generates problems instead of solutions - that's perhaps future work. Another suggestion was to integrate with a formula editor for things like asking about structures by drawing them (beyond scope, but interesting). Overall, in this limited test, the users felt that the tool may be a helpful study aid particularly for reviewing concepts.
Quantitative Evaluation of Retrieval
As part of evaluation, we also looked at how well the retrieval component is working by looking at recall: for a set of questions, we manually identified which chunk of the textbook would have the answer to the question, and checked if that chunk was among the top-k retrieved by FAISS. We found that on the majority of the direct concept questions, the correct chunk was ranked #1. In a few cases of more complex questions (e.g., "Why is the carbocation intermediate planar?"), the info asked for was split across two chunks and the retriever got one of them but not the other because of differences in wording. Incrementing k by 5 might just have grabbed it, if not for more irrelevant context. Since the LMM sees all retrieved chunks, having too many unrelated chunks could confuse it or fill up context window. We kept k=4 as a trade off and that seems to capture the needed info in >90% of tested queries. The quality of the embedding model is clearly visible here - if we had used a weaker embedding, it might miss some key info on the retrieval. The nomic-embed-text model actually did work; it probably helped that it was trained on long contexts and so would be good at matching even if the phrasing in queries and textbooks is different.
Overall Assessment
Putting together all the above points of evaluation, we can claim the following:
The system meets the fundamental goals: It delivers accurate and grounded explanations (thanks to RAG); interacts with voice and visuals (thanks to Web Speech and RDKit); and runs locally, without violating privacy and without internet connection after setup.
Users find it useful and user-friendly with minor improvements suggested mostly around extended functionality.
In terms of academic performance one could imagine a measure of learning outcomes by asking students to use the tutor vs not the tutor, but that's outside our current scope. However, based on how clearly and correctly the answers are, we expect it that it would be a positive supplement to learning rather than a source of confusion or misinformation.
One limit seen is that the system is only as good as the material that is provided. If there's some inaccuracy in the text book, or an out dated concept the tutor would propagate that. Also, if a student asks something that requires reasoning that goes beyond simply fetching some snippet (like a novel problem that combines concepts), the system may struggle. LLMs are capable of some reasoning, but our strict grounding could make the model reluctant to move beyond a given context. This is one known tension in RAG: a tradeoff between factual grounding versus the creative reasoning ability of the model. In our tests, some "Why" questions had answers that came right from the text (which is not necessarily good enough for "why" - sometimes the textbook mentions a fact but not the reason for it). The model didn't make up a reason - it stuck to context. In future, maybe we will be able to augment the context by some inference or some chain-of-thought from the model itself.
Another limit is that the voice feature depends on the browser; this effectively means for mobile devices it may not work (some mobile browsers disallow continuous listening). An ideal implementation could be as a mobile application and using native speech APIs for greater device support.
Conclusion
In this dissertation, we presented the development of ChemTutor, a voice interactive bilingual tutor for organic chemistry that combines a local large language model with a retrieval-based knowledge system and domain-specific drawing capabilities. The project is a demonstration of how modern artificial intelligence techniques can be used to build a rich tutoring experience in a specialized subject area:
We demonstrated that when using Retrieval-Augmented Generation (RAG), answers from the system are still based on correct and corrective information (in our case, a textbook). Having a knowledge base helps the LLM to hallucinate less and give an overall correct response [4]. From AI assistance to traditional learning resources, the tutor not only provides answers but provides citations and references.
The inclusion of voice in/out makes learning more accessible and more natural. Students have the opportunity to have conversation with the tutor as they would in a normal class with a real teacher, which is especially helpful for those who learn better when spoken to or have mobility/vision deficits that make text-based interaction less convenient. Utilizing the Web Speech API meant that this is something we can implement fully on-device[8], and that the design can be improved further in the future with more sophisticated TTS voices.
By involving very little code-switching to the Nepali language, we addressed the local context of Nepali students. Having this little experience of a touch of bilingual instruction [2] in the app makes it more human and in touch with students in Nepal contextually which leads to more comfortable and engaging user experience for the students.
The addition of RDKit for drawing molecular structure brought a visual component to the tutor's capabilities. Organic chemistry requires a great deal of visualizing of molecules, and our system addresses that by actually creating the chemical structure diagrams on the fly in response to a user query. This is a feature that transforms the learning experience as students are not only able to hear or read about a molecule, they are able to see it - an important part of understanding chemistry. The technical approach of the LLM to respond with JSON format and call the RDKit tool for the diagram was a good solution. It opens up possibilities for the addition of more tools (imagine mechanism animations or 3D models) in the same framework.
Our system is implemented totally local with open source components (Mistral LLM, Nomic embed, FAISS, RDKit), but the main point is to demonstrate the feasibility of privacy-friendly AI applications. The performance of the models we chose was sufficient to work interactively and the quality was not inferior to what you'd expect from cloud-based solutions for this task at hand. This is an interesting aspect for educators or institutions that worry about data privacy, internet connection or recurring costs: a local solution such as ChemTutor can be implemented in a lab or classroom, without having an internet connection and still benefit from state-of-the-art AI help.
In evaluating ChemTutor, we found that it is an accurate and clear means of offering explanation for a wide range of queries in organic chemistry and does so in a user-friendly way. The tutor taps into multiple modalities - text, audio, visual - to appeal to various learning preferences. Early user testing suggested that students like the system's capacity to quickly explain concepts and display visuals, the equivalent of a 24/7 personal tutor or study buddy.
There are of course spaces for improvement and areas of work to be done in the future. Some of these include:
Expanded Knowledge Base: Currently the system's knowledge base is constrained to whatever we have indexed (one textbook basically). You can expand it further to include additional textbooks, lecture notes, or even allow the user to upload their class notes/slides. However, as the amount of data to be retrieved increases, the challenge becomes to make certain that the retrieval is in context and not swamping the LLM with too much context. However, if we use very large corpora, then some form of clustering or filtering is necessary.
Larger or Specialized Models: While Mistral 7B did a great job (and impressively matched the performance of a 13B model in many ways), a larger model that has been fine-tuned specifically for tutoring could potentially provide even fluenter or more instructive responses. If resource allows, a 13B or 30B parameter model can be attempted for more depth in the answer and hopefully a better treatment of multi-turn context. There are also chemistry-specific models (e.g., those fine-tuned on science QA data sets or incorporating chemistry knowledge) that could be investigated. The modularity of the system allows us to change the model if desired.
Interactive Teaching Strategies: Currently the tutor is doing mostly Q&A. However, with more interactive pedagogy, such as asking the questions back to the student (Socratic method) and leading them to the answer, or analogies and mnemonics for difficult concepts it might be improved. This may require engineering or adjusting the model to use the teaching strategies immediately after asking a question.
Assessment and Quizzing Mode: A nice addition would be for the tutor to be able to quiz the student. For example, after a concept has been explained, a follow-up question may be asked to verify understanding. Or it could produce practice problems on the fly (e.g., "Identify the electrophile in this reaction: ..."). This represents a shift from the answering to the teaching and assessing, which would need to be carefully controlled, to ensure that only valid questions are created. Some work has been done with LLMs creating educational material, which we could piggyback off.
Multilingual Support: Apart from the Nepali phrases, one can imagine fully supporting the question asked in Nepali or other languages and answering in the same language (or bilingual-wise). Because of hardware limitations, our embedding model as well as the LLM is trained largely on English language, having an overall good Nepali dialect and error free response will not be achievable in our current architecture. With a multilingual embedding model and adding a translated knowledge base, we will most likely be able to answer Nepali questions. This can be useful for situations when students are more comfortable asking in their L1.
Voice Integration Enhancement: Having a fine tuned TTS model (as discussed, via API or offline neural voices) and a more intelligent speech recognition with a specialized vocabulary for technical chemistry terminology can improve the quality of voice interaction with the students. Also, on the client-side, use voice activity detection to auto-stop listening when the user is finished speaking (now we just use the auto-end detection provided by SpeechRecognition, which is actually quite decent, but sometimes cuts off or waits).
User Interface Enhancements: Simple gestures like highlighting the text as it's being read (karaoke style) can enhance the user experience. Also, you may want to let users easily copy answers, or save an answer to a notes list, for studying. Another possibility is to include a "confidence" indicator, or a specific statement whenever the tutor was unsure (perhaps given by a lower similarity score) but our design tends to just output that it can't find something when it is unsure.
Evaluation in Classroom Settings: As the next step, it would be helpful to actually use ChemTutor in a classroom or for a semester and gather feedback and outcomes for learning assessment. Our evaluation was small in scale, but one of the next steps would be a more rigorous study to determine if students' use of the tutor results in higher understanding or grades, while collecting student and teacher impressions.
In summary, this project shows a successful combination of cutting-edge AI with education technology for STEM. By integrating AI into the existing curriculum and bolstering the interaction with voice and visuals, we have developed an innovative yet practical tool. As AI technology continues to advance, these sorts of tutors may become more widespread, making individualized learning more accessible to everyone. ChemTutor is aimed particularly at the niche of organic chemistry, but the idea is broadly applicable-a similar system can be envisaged for physics (drawing of diagrams of circuits or free-body diagrams), biology (images of the structure of cells), mathematics (rendering equations and drawing graphs), etc. The main lessons learned from this project - grounding, multimodal output and local deployment - can guide those future efforts.
Ultimately, the synergy between AI and education is a promising one. This project represents a move in that direction, setting an example of how a carefully designed synergy of retrieval systems and language generation can lead to an educational tool that is knowledgeable, reliable and interactive. The encouraging results provide confidence that such systems will be able to augment learning experience, and potentially even remove from human educators some of the mundane queries and explanations that currently take up time, freeing teachers to focus on more advanced mentoring.
ChemTutor was originally created to make concepts in organic chemistry easier for students to learn. The results of this project indicate that in fact it can, and that there are many possibilities for further refinement and application within education. "As we iterate these systems and collect more feedback, we get closer to a day when every student can have their own AI tutor that adapts to their needs on demand - a dream many have held in education for a long time, and one that is now quite real thanks to the advances in AI today."

References
MDN Web Docs (2025) Web Speech API. Available at: https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API (Accessed: 03 September 2025).
MDN Web Docs (2025) SpeechRecognition — Web APIs. Available at: https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition (Accessed: 09 September 2025).
MDN Web Docs (2025) SpeechSynthesis — Web APIs. Available at: https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis (Accessed: 21 August 2025).
Mistral AI (2023) Announcing Mistral 7B. Available at: https://mistral.ai/news/announcing-mistral-7b (Accessed: 14 September 2025).
Jiang, A.Q. et al. (2023) ‘Mistral 7B’, arXiv preprint arXiv:2310.06825. Available at: https://arxiv.org/abs/2310.06825 (Accessed: 30 August 2025).
Meta FAIR (2025) Faiss — documentation homepage. Available at: https://faiss.ai (Accessed: 18 September 2025).
Meta FAIR (2025) facebookresearch/faiss — GitHub repository. Available at: https://github.com/facebookresearch/faiss (Accessed: 27 August 2025).
RDKit Project (2025) Getting Started with the RDKit in Python. Available at: https://rdkit.readthedocs.io/en/latest/GettingStartedInPython.html (Accessed: 08 September 2025).
RDKit Project (2025) The RDKit Book. Available at: https://www.rdkit.org/docs/RDKit_Book.html (Accessed: 02 September 2025).
Lewis, P. et al. (2020) ‘Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks’, arXiv preprint arXiv:2005.11401. Available at: https://arxiv.org/abs/2005.11401 (Accessed: 11 September 2025).
Lewis, P. et al. (2020) ‘Retrieval-augmented generation for knowledge-intensive NLP tasks’, Proceedings of NeurIPS 2020. Available at: https://dl.acm.org/doi/abs/10.5555/3495724.3496517 (Accessed: 25 August 2025).
Nomic AI (2024) Introducing Nomic Embed: A Truly Open Embedding Model (nomic-embed-text v1). Available at: https://www.nomic.ai/blog/posts/nomic-embed-text-v1 (Accessed: 06 September 2025).
Nomic AI (2024) nomic-embed-text-v1 — Hugging Face model card. Available at: https://huggingface.co/nomic-ai/nomic-embed-text-v1 (Accessed: 31 August 2025).
Zhang, Y. et al. (2024) ‘Retrieval-Augmented Generation for Natural Language Processing: A Survey’, arXiv preprint arXiv:2407.13193. Available at: https://arxiv.org/html/2407.13193v1 (Accessed: 16 September 2025).
Sah, P.K. (2022) ‘Translanguaging or unequal languaging? Teachers’ and students’ language practices in EMI classrooms in Nepal’, International Journal of Bilingual Education and Bilingualism, 25(7), pp. 2561–2577. Available at: https://www.tandfonline.com/doi/full/10.1080/13670050.2020.1849011 (Accessed: 29 August 2025).
Sirisathitkul, C. and Jaroonchokanan, N. (2025) ‘Implementing ChatGPT as Tutor, Tutee, and Tool in Physics and Chemistry’, Substantia, 9(1), pp. 89–101. Available at: https://riviste.fupress.net/index.php/subs/article/view/2808 (Accessed: 12 September 2025).
